"""
Machine Learning - PrevisÃ£o IBOVESPA
Objetivo: Usar REGRESSÃƒO para prever retorno do prÃ³ximo dia e converter em direÃ§Ã£o
Meta: AcurÃ¡cia mÃ­nima de 75% no conjunto de teste (Ãºltimos 30 dias)
Abordagem: RegressÃ£o Linear + conversÃ£o para classificaÃ§Ã£o binÃ¡ria
"""

# IMPORTS NECESSÃRIOS
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Scikit-Learn
from sklearn.model_selection import train_test_split, cross_val_score, KFold, TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score, make_scorer, accuracy_score
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import VotingRegressor
import matplotlib.pyplot as plt
import seaborn as sns

def carregar_dados_ibovespa(anos=10):
    """
    ETAPA 1: CARREGAMENTO DE DADOS
    Baixa dados histÃ³ricos do IBOVESPA via Yahoo Finance
    """
    print("ETAPA 1: Carregando dados do IBOVESPA...")
    
    # Definir perÃ­odo
    end_date = datetime.now()
    start_date = end_date - timedelta(days=anos * 365)
    
    # Baixar dados
    data = yf.download('^BVSP', start=start_date, end=end_date)
    
    # Limpar colunas se necessÃ¡rio
    if isinstance(data.columns, pd.MultiIndex):
        data.columns = data.columns.get_level_values(0)
    
    print(f"âœ“ {len(data)} dias carregados ({anos} anos)")
    
    return data

def criar_features(data):
    """
    ETAPA 2: CRIAÃ‡ÃƒO DE FEATURES
    Cria indicadores tÃ©cnicos para o modelo
    """
    print("ETAPA 2: Criando features tÃ©cnicas...")
    
    # Calcular retornos
    data['Retorno'] = data['Close'].pct_change()
    
    # MÃ©dias mÃ³veis
    data['MM5'] = data['Close'].rolling(5).mean()
    data['MM20'] = data['Close'].rolling(20).mean()
    data['MM50'] = data['Close'].rolling(50).mean()
    
    # RSI (Ãndice de ForÃ§a Relativa)
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / loss
    data['RSI'] = 100 - (100 / (1 + rs))
    
    # Volatilidade
    data['Volatilidade'] = data['Retorno'].rolling(20).std()
    
    # Volume normalizado
    data['Volume_Norm'] = data['Volume'] / data['Volume'].rolling(20).mean()
    
    # PosiÃ§Ã£o no canal
    data['Max_20'] = data['High'].rolling(20).max()
    data['Min_20'] = data['Low'].rolling(20).min()
    data['Posicao_Canal'] = (data['Close'] - data['Min_20']) / (data['Max_20'] - data['Min_20'])
    
    # Indicadores de momentum
    data['Momentum_3'] = data['Close'] / data['Close'].shift(3) - 1
    data['Momentum_7'] = data['Close'] / data['Close'].shift(7) - 1
    data['Retorno_Acum_3'] = data['Retorno'].rolling(3).sum()
    
    # Sinais de cruzamento
    data['MM_Cross'] = (data['MM5'] > data['MM20']).astype(int)
    data['Price_Above_MA20'] = (data['Close'] > data['MM20']).astype(int)
    
    # Sinais de RSI
    data['RSI_Signal'] = np.where(data['RSI'] < 30, 1, np.where(data['RSI'] > 70, -1, 0))
    
    # Retornos de diferentes perÃ­odos
    data['Returns_3d'] = data['Close'].pct_change(3)
    data['Returns_7d'] = data['Close'].pct_change(7)
    
    # AnÃ¡lise de volume
    data['Volume_Spike'] = (data['Volume'] > 1.5 * data['Volume'].rolling(20).mean()).astype(int)
    
    # Regime de volatilidade
    data['Low_Vol'] = (data['Volatilidade'] < data['Volatilidade'].rolling(50).quantile(0.3)).astype(int)
    
    # PosiÃ§Ã£o no canal de preÃ§os
    data['Max_20'] = data['High'].rolling(20).max()
    data['Min_20'] = data['Low'].rolling(20).min()
    data['Channel_Position'] = (data['Close'] - data['Min_20']) / (data['Max_20'] - data['Min_20'])
    
    features_lista = [
        'MM5', 'MM20', 'RSI', 'Volatilidade',
        'MM_Cross', 'Price_Above_MA20', 'RSI_Signal',
        'Returns_3d', 'Returns_7d', 'Volume_Spike', 'Low_Vol', 'Channel_Position'
    ]
    
    print(f"âœ“ {len(features_lista)} features criadas")
    
    return data, features_lista

def criar_target(data):
    """
    ETAPA 3: CRIAÃ‡ÃƒO DO TARGET PARA REGRESSÃƒO
    Target: retorno percentual do prÃ³ximo dia (para regressÃ£o)
    """
    print("ETAPA 3: Criando target para regressÃ£o...")
    
    # Target: retorno do prÃ³ximo dia (em percentual)
    data['Target_Return'] = data['Close'].pct_change().shift(-1)
    
    # Para anÃ¡lise tambÃ©m criar a direÃ§Ã£o (mas nÃ£o usar no treino)
    data['Target_Direction'] = (data['Target_Return'] > 0).astype(int)
    
    # EstatÃ­sticas bÃ¡sicas
    returns_stats = data['Target_Return'].describe()
    direction_counts = data['Target_Direction'].value_counts()
    total = len(data['Target_Direction'].dropna())
    
    print(f"âœ“ Target de REGRESSÃƒO criado:")
    print(f"  Retorno mÃ©dio: {returns_stats['mean']:.4f}")
    print(f"  Desvio padrÃ£o: {returns_stats['std']:.4f}")
    print(f"  Min: {returns_stats['min']:.4f} | Max: {returns_stats['max']:.4f}")
    print(f"  Dias de alta: {direction_counts.get(1, 0)} ({direction_counts.get(1, 0)/total:.1%})")
    print(f"  Dias de baixa: {direction_counts.get(0, 0)} ({direction_counts.get(0, 0)/total:.1%})")
    
    return data

def preparar_dataset(data, features_lista):
    """
    ETAPA 4: PREPARAÃ‡ÃƒO DO DATASET PARA REGRESSÃƒO
    Limpa os dados e separa features do target - Ãšltimos 30 dias para teste
    """
    print("ETAPA 4: Preparando dataset para regressÃ£o...")
    
    # Criar dataset final usando target de regressÃ£o
    dataset = data[features_lista + ['Target_Return', 'Target_Direction']].copy()
    
    # Remover valores nulos
    dataset = dataset.dropna()
    
    # DIVISÃƒO ESPECÃFICA: Ãšltimos 30 dias para teste, resto para treino
    n_test_days = 30
    split_index = len(dataset) - n_test_days
    
    # Separar dados de treino e teste
    data_treino = dataset.iloc[:split_index]
    data_teste = dataset.iloc[split_index:]
    
    # Separar X (features) e y (target de regressÃ£o)
    X_treino = data_treino[features_lista]
    y_treino = data_treino['Target_Return']  # Target de regressÃ£o
    X_teste = data_teste[features_lista]
    y_teste = data_teste['Target_Return']     # Target de regressÃ£o
    
    # TambÃ©m separar as direÃ§Ãµes reais para avaliar acurÃ¡cia
    y_treino_dir = data_treino['Target_Direction']
    y_teste_dir = data_teste['Target_Direction']
    
    print(f"âœ“ Dataset preparado para regressÃ£o:")
    print(f"  Total de observaÃ§Ãµes: {len(dataset)}")
    print(f"  Dados de treino: {len(X_treino)} observaÃ§Ãµes")
    print(f"  Dados de teste (Ãºltimos 30 dias): {len(X_teste)} observaÃ§Ãµes")
    print(f"  Features: {len(features_lista)}")
    
    return X_treino, X_teste, y_treino, y_teste, y_treino_dir, y_teste_dir

def validacao_cruzada_regressao(X_treino, y_treino, y_treino_dir):
    """
    ETAPA 5: VALIDAÃ‡ÃƒO CRUZADA COM REGRESSÃƒO
    Testa mÃºltiplos modelos de regressÃ£o e converte previsÃµes em direÃ§Ã£o
    """
    print("ETAPA 5: ValidaÃ§Ã£o cruzada com modelos de REGRESSÃƒO...")
    
    # Modelos de regressÃ£o para testar
    modelos = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0, random_state=42),
        'Lasso Regression': Lasso(alpha=0.1, random_state=42),
        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
    }
    
    # FunÃ§Ã£o personalizada para calcular acurÃ¡cia de direÃ§Ã£o a partir de regressÃ£o
    def acuracia_direcao_regressao(y_true, y_pred):
        """Converte previsÃµes de regressÃ£o em direÃ§Ã£o e calcula acurÃ¡cia"""
        direcao_real = (y_true > 0).astype(int)
        direcao_pred = (y_pred > 0).astype(int)
        return np.mean(direcao_real == direcao_pred)
    
    # Criar scorer personalizado
    scorer_acuracia = make_scorer(acuracia_direcao_regressao)
    
    resultados = {}
    
    # Testar cada modelo
    for nome, modelo_base in modelos.items():
        # Criar pipeline com normalizaÃ§Ã£o
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('regressor', modelo_base)
        ])
        
        # ValidaÃ§Ã£o cruzada (5 folds) sem shuffle para manter ordem temporal
        cv = KFold(n_splits=5, shuffle=False)
        
        # Calcular acurÃ¡cia de direÃ§Ã£o usando regressÃ£o
        scores_acuracia = cross_val_score(
            pipeline, X_treino, y_treino, cv=cv, scoring=scorer_acuracia, n_jobs=-1
        )
        
        # TambÃ©m calcular RÂ² para avaliar qualidade da regressÃ£o
        scores_r2 = cross_val_score(
            pipeline, X_treino, y_treino, cv=cv, scoring='r2', n_jobs=-1
        )
        
        # Armazenar resultados
        resultados[nome] = {
            'acuracia_mean': scores_acuracia.mean(),
            'acuracia_std': scores_acuracia.std(),
            'r2_mean': scores_r2.mean(),
            'r2_std': scores_r2.std(),
            'scores': scores_acuracia
        }
        
        # Mostrar resultados
        status = "âœ…" if scores_acuracia.mean() >= 0.75 else "âš ï¸" if scores_acuracia.mean() >= 0.60 else "âŒ"
        print(f"  {status} {nome}:")
        print(f"     AcurÃ¡cia direÃ§Ã£o: {scores_acuracia.mean():.1%} (Â±{scores_acuracia.std():.1%})")
        print(f"     RÂ² regressÃ£o: {scores_r2.mean():.4f} (Â±{scores_r2.std():.4f})")
    
    # Encontrar melhor modelo
    melhor_modelo_nome = max(resultados.keys(), key=lambda k: resultados[k]['acuracia_mean'])
    melhor_score = resultados[melhor_modelo_nome]['acuracia_mean']
    
    print(f"\nğŸ† Melhor modelo: {melhor_modelo_nome}")
    print(f"   AcurÃ¡cia: {melhor_score:.1%}")
    print(f"   RÂ²: {resultados[melhor_modelo_nome]['r2_mean']:.4f}")
    
    if melhor_score >= 0.75:
        print("âœ… META ATINGIDA: AcurÃ¡cia >= 75% na validaÃ§Ã£o cruzada!")
    else:
        print(f"âš ï¸ META NÃƒO ATINGIDA: AcurÃ¡cia {melhor_score:.1%} < 75%")
        print(f"   Faltam {(0.75 - melhor_score)*100:.1f} pontos percentuais")
    
    return resultados, melhor_modelo_nome


def teste_final_30_dias(X_treino, X_teste, y_treino, y_teste, y_teste_dir, modelo_nome):
    """
    ETAPA 6: TESTE FINAL NOS ÃšLTIMOS 30 DIAS COM REGRESSÃƒO
    Avalia o melhor modelo de regressÃ£o nos Ãºltimos 30 dias
    """
    print("ETAPA 6: Teste final nos Ãºltimos 30 dias com REGRESSÃƒO...")
    
    # Mapear nome do modelo para classe (REGRESSÃƒO)
    if modelo_nome == "Ensemble":
        modelo = criar_ensemble_robusto()
    else:
        modelos_map = {
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=1.0, random_state=42),
            'Lasso Regression': Lasso(alpha=0.1, random_state=42),
            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
        }
        modelo = modelos_map.get(modelo_nome, LinearRegression())
    
    # Criar pipeline com o modelo
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', modelo)
    ])
    
    # Treinar no conjunto de treino (usando retornos)
    pipeline.fit(X_treino, y_treino)
    
    # Fazer previsÃµes no conjunto de teste (Ãºltimos 30 dias)
    y_pred_returns = pipeline.predict(X_teste)
    
    # Converter previsÃµes de regressÃ£o para direÃ§Ã£o
    y_pred_direction = (y_pred_returns > 0).astype(int)
    
    # Calcular acurÃ¡cia usando as direÃ§Ãµes
    acuracia_final = accuracy_score(y_teste_dir, y_pred_direction)
    
    print(f"âœ“ Modelo usado: {modelo_nome}")
    print(f"âœ“ AcurÃ¡cia nos Ãºltimos 30 dias: {acuracia_final:.1%}")
    
    # Verificar se atingiu a meta
    if acuracia_final >= 0.75:
        print("ğŸ‰ META ATINGIDA: AcurÃ¡cia >= 75% no conjunto de teste!")
        status = "SUCESSO"
    else:
        print("âŒ META NÃƒO ATINGIDA: AcurÃ¡cia < 75% no conjunto de teste")
        status = "FALHA"
    
    # RelatÃ³rio detalhado
    print(f"\nğŸ“Š RelatÃ³rio detalhado do teste:")
    print(f"  PrevisÃµes corretas: {(y_pred_direction == y_teste_dir).sum()}/{len(y_teste_dir)}")
    print(f"  Baseline (sempre classe majoritÃ¡ria): {max(y_teste_dir.mean(), 1-y_teste_dir.mean()):.1%}")
    
    # DistribuiÃ§Ã£o das previsÃµes
    pred_counts = pd.Series(y_pred_direction).value_counts()
    real_counts = y_teste_dir.value_counts()
    print(f"  PrevisÃµes ALTA: {pred_counts.get(1, 0)} | Real ALTA: {real_counts.get(1, 0)}")
    print(f"  PrevisÃµes BAIXA: {pred_counts.get(0, 0)} | Real BAIXA: {real_counts.get(0, 0)}")
    
    # EstatÃ­sticas da regressÃ£o
    mse = mean_squared_error(y_teste, y_pred_returns)
    r2 = r2_score(y_teste, y_pred_returns)
    print(f"\nğŸ“ˆ Qualidade da regressÃ£o:")
    print(f"  MSE: {mse:.6f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  Retorno real mÃ©dio: {y_teste.mean():.4f}")
    print(f"  Retorno previsto mÃ©dio: {y_pred_returns.mean():.4f}")
    
    return {
        'acuracia': acuracia_final,
        'status': status,
        'modelo': modelo_nome,
        'predicoes_retorno': y_pred_returns,
        'predicoes_direcao': y_pred_direction,
        'real_retorno': y_teste,
        'real_direcao': y_teste_dir,
        'mse': mse,
        'r2': r2
    }


def analisar_correlacoes_features(data, features_lista, target_col='Target_Return'):
    """
    ANÃLISE DETALHADA DE CORRELAÃ‡Ã•ES
    Analisa correlaÃ§Ãµes entre features e target para identificar as mais relevantes
    """
    print("="*60)
    print("ANÃLISE DETALHADA DE CORRELAÃ‡Ã•ES")
    print("="*60)
    
    # Criar dataset completo para anÃ¡lise
    dataset = data[features_lista + [target_col, 'Target_Direction']].copy()
    dataset = dataset.dropna()
    
    print(f"ğŸ“Š Analisando {len(features_lista)} features com {len(dataset)} observaÃ§Ãµes")
    
    # 1. CORRELAÃ‡ÃƒO COM TARGET DE REGRESSÃƒO
    print(f"\nğŸ¯ CORRELAÃ‡ÃƒO COM RETORNO (Target de RegressÃ£o):")
    correlacoes_retorno = dataset[features_lista].corrwith(dataset[target_col]).abs().sort_values(ascending=False)
    print("Top 10 features mais correlacionadas com o retorno:")
    for i, (feature, corr) in enumerate(correlacoes_retorno.head(10).items(), 1):
        status = "ğŸŸ¢" if corr > 0.05 else "ğŸŸ¡" if corr > 0.02 else "ğŸ”´"
        print(f"  {i:2d}. {status} {feature:<20}: {corr:.4f}")
    
    # 2. CORRELAÃ‡ÃƒO COM TARGET DE DIREÃ‡ÃƒO
    print(f"\nğŸ¯ CORRELAÃ‡ÃƒO COM DIREÃ‡ÃƒO (Target de ClassificaÃ§Ã£o):")
    correlacoes_direcao = dataset[features_lista].corrwith(dataset['Target_Direction']).abs().sort_values(ascending=False)
    print("Top 10 features mais correlacionadas com a direÃ§Ã£o:")
    for i, (feature, corr) in enumerate(correlacoes_direcao.head(10).items(), 1):
        status = "ğŸŸ¢" if corr > 0.05 else "ğŸŸ¡" if corr > 0.02 else "ğŸ”´"
        print(f"  {i:2d}. {status} {feature:<20}: {corr:.4f}")
    
    # 3. MATRIZ DE CORRELAÃ‡ÃƒO ENTRE FEATURES
    print(f"\nğŸ”— CORRELAÃ‡ÃƒO ENTRE FEATURES (detectar multicolinearidade):")
    corr_matrix = dataset[features_lista].corr().abs()
    
    # Encontrar pares altamente correlacionados
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > 0.8:
                high_corr_pairs.append((
                    corr_matrix.columns[i], 
                    corr_matrix.columns[j], 
                    corr_matrix.iloc[i, j]
                ))
    
    if high_corr_pairs:
        print("âš ï¸ Features altamente correlacionadas (>0.8) - possÃ­vel multicolinearidade:")
        for feat1, feat2, corr in high_corr_pairs:
            print(f"   {feat1} <-> {feat2}: {corr:.3f}")
    else:
        print("âœ… NÃ£o hÃ¡ multicolinearidade significativa entre features")
    
    # 4. SELEÃ‡ÃƒO DE MELHORES FEATURES
    print(f"\nğŸ¯ SELEÃ‡ÃƒO DE FEATURES BASEADA EM CORRELAÃ‡ÃƒO:")
    
    # Combinar correlaÃ§Ãµes (mÃ©dia ponderada)
    correlacao_combinada = (correlacoes_retorno * 0.6 + correlacoes_direcao * 0.4).sort_values(ascending=False)
    
    # Selecionar top features
    top_5_features = correlacao_combinada.head(5).index.tolist()
    top_8_features = correlacao_combinada.head(8).index.tolist()
    top_all_features = correlacao_combinada.index.tolist()
    
    print(f"ğŸ¥‡ TOP 5 Features (correlaÃ§Ã£o combinada):")
    for i, feature in enumerate(top_5_features, 1):
        corr_comb = correlacao_combinada[feature]
        corr_ret = correlacoes_retorno[feature]
        corr_dir = correlacoes_direcao[feature]
        print(f"  {i}. {feature:<20}: {corr_comb:.4f} (ret:{corr_ret:.4f}, dir:{corr_dir:.4f})")
    
    print(f"\nğŸ¥ˆ TOP 8 Features (correlaÃ§Ã£o combinada):")
    for i, feature in enumerate(top_8_features, 1):
        corr_comb = correlacao_combinada[feature]
        print(f"  {i}. {feature:<20}: {corr_comb:.4f}")
    
    # 5. ESTATÃSTICAS DESCRITIVAS DAS MELHORES FEATURES
    print(f"\nğŸ“ˆ ESTATÃSTICAS DAS TOP 5 FEATURES:")
    top_5_stats = dataset[top_5_features].describe()
    print(top_5_stats.round(4))
    
    return {
        'correlacoes_retorno': correlacoes_retorno,
        'correlacoes_direcao': correlacoes_direcao,
        'correlacao_combinada': correlacao_combinada,
        'top_5_features': top_5_features,
        'top_8_features': top_8_features,
        'high_corr_pairs': high_corr_pairs,
        'corr_matrix': corr_matrix,
        'dataset_analise': dataset
    }

def criar_features_avancadas(data):
    """
    CRIAÃ‡ÃƒO DE FEATURES AVANÃ‡ADAS
    Baseado na anÃ¡lise de correlaÃ§Ã£o, criar features mais sofisticadas
    """
    print("\n" + "="*60)
    print("CRIAÃ‡ÃƒO DE FEATURES AVANÃ‡ADAS")
    print("="*60)
    
    # Features originais jÃ¡ criadas
    data, features_originais = criar_features(data)
    
    print("ğŸš€ Criando features avanÃ§adas baseadas em anÃ¡lise de correlaÃ§Ã£o...")
    
    # 1. FEATURES DE MOMENTUM AVANÃ‡ADAS
    data['Momentum_Ratio'] = data['MM5'] / data['MM20']  # RazÃ£o entre mÃ©dias
    data['Price_Momentum'] = data['Close'] / data['MM50']  # PosiÃ§Ã£o relativa Ã  MM50
    data['Volume_Price_Momentum'] = data['Volume_Norm'] * data['Retorno']  # Volume ponderado pelo retorno
    
    # 2. FEATURES DE VOLATILIDADE AVANÃ‡ADAS
    data['Volatilidade_Relativa'] = data['Volatilidade'] / data['Volatilidade'].rolling(50).mean()
    data['Volatilidade_Tendencia'] = data['Volatilidade'].rolling(5).mean() / data['Volatilidade'].rolling(20).mean()
    
    # 3. FEATURES DE RSI AVANÃ‡ADAS
    data['RSI_Momentum'] = data['RSI'].diff()  # Momentum do RSI
    data['RSI_Volatilidade'] = data['RSI'].rolling(10).std()  # Volatilidade do RSI
    data['RSI_Normalized'] = (data['RSI'] - 50) / 50  # RSI normalizado entre -1 e 1
    
    # 4. FEATURES DE CANAL/POSIÃ‡ÃƒO AVANÃ‡ADAS
    data['Canal_Momentum'] = data['Channel_Position'].diff()  # Movimento no canal
    data['Breakout_Signal'] = ((data['Close'] > data['Max_20']) | (data['Close'] < data['Min_20'])).astype(int)
    
    # 5. FEATURES COMBINADAS (INTERAÃ‡Ã•ES)
    data['RSI_Volume'] = data['RSI_Normalized'] * data['Volume_Norm']  # RSI ponderado por volume
    data['Momentum_Volatilidade'] = data['Momentum_3'] / (data['Volatilidade'] + 0.001)  # Momentum ajustado por volatilidade
    data['MM_Cross_Strength'] = (data['MM5'] - data['MM20']) / data['MM20']  # ForÃ§a do cruzamento
    
    # 6. FEATURES DE REGIME DE MERCADO
    data['Bull_Market'] = (data['Close'] > data['MM50']).astype(int)  # Mercado em alta
    data['High_Vol_Regime'] = (data['Volatilidade'] > data['Volatilidade'].rolling(50).quantile(0.7)).astype(int)
    data['Consolidation'] = ((data['Max_20'] - data['Min_20']) / data['Close'] < 0.05).astype(int)  # ConsolidaÃ§Ã£o
    
    # 7. FEATURES LAGGED (DEFASADAS)
    data['Retorno_Lag1'] = data['Retorno'].shift(1)
    data['Retorno_Lag2'] = data['Retorno'].shift(2)
    data['RSI_Lag1'] = data['RSI'].shift(1)
    data['Volume_Norm_Lag1'] = data['Volume_Norm'].shift(1)
    
    # 8. FEATURES DE TENDÃŠNCIA
    data['Tendencia_5d'] = (data['Close'] > data['Close'].shift(5)).astype(int)
    data['Tendencia_10d'] = (data['Close'] > data['Close'].shift(10)).astype(int)
    data['Aceleracao'] = data['Retorno'] - data['Retorno'].shift(1)  # AceleraÃ§Ã£o do retorno
    
    # Lista completa de features
    features_avancadas = [
        # Features originais
        'MM5', 'MM20', 'RSI', 'Volatilidade', 'MM_Cross', 'Price_Above_MA20', 
        'RSI_Signal', 'Returns_3d', 'Returns_7d', 'Volume_Spike', 'Low_Vol', 'Channel_Position',
        
        # Features avanÃ§adas
        'Momentum_Ratio', 'Price_Momentum', 'Volume_Price_Momentum',
        'Volatilidade_Relativa', 'Volatilidade_Tendencia',
        'RSI_Momentum', 'RSI_Volatilidade', 'RSI_Normalized',
        'Canal_Momentum', 'Breakout_Signal',
        'RSI_Volume', 'Momentum_Volatilidade', 'MM_Cross_Strength',
        'Bull_Market', 'High_Vol_Regime', 'Consolidation',
        'Retorno_Lag1', 'Retorno_Lag2', 'RSI_Lag1', 'Volume_Norm_Lag1',
        'Tendencia_5d', 'Tendencia_10d', 'Aceleracao'
    ]
    
    print(f"âœ… {len(features_avancadas)} features criadas ({len(features_avancadas) - len(features_originais)} novas)")
    print(f"   Features originais: {len(features_originais)}")
    print(f"   Features adicionadas: {len(features_avancadas) - len(features_originais)}")
    
    return data, features_avancadas

def main():
    """
    FUNÃ‡ÃƒO PRINCIPAL
    Executa pipeline completo com anÃ¡lise de correlaÃ§Ã£o e seleÃ§Ã£o otimizada de features
    """
    print("="*70)
    print("MACHINE LEARNING - PREVISÃƒO IBOVESPA")
    print("ABORDAGEM: REGRESSÃƒO + ANÃLISE DE CORRELAÃ‡ÃƒO")
    print("META: 75% de acurÃ¡cia nos Ãºltimos 30 dias")
    print("="*70)
    
    try:
        # 1. Carregar dados
        data = carregar_dados_ibovespa(anos=10)
        
        # 2. Criar features avanÃ§adas (incluindo as originais)
        data, features_completas = criar_features_avancadas(data)
        
        # 3. Criar target
        data = criar_target(data)
        
        # 4. ANÃLISE DE CORRELAÃ‡ÃƒO DETALHADA
        print("\nğŸ” Executando anÃ¡lise detalhada de correlaÃ§Ãµes...")
        analise_correlacao = analisar_correlacoes_features(data, features_completas)
        
        # 5. Preparar dataset com divisÃ£o especÃ­fica
        X_treino, X_teste, y_treino, y_teste, y_treino_dir, y_teste_dir = preparar_dataset(data, features_completas)
        
        # 6. CORREÃ‡ÃƒO DE MULTICOLINEARIDADE
        print("\nğŸ”§ Aplicando correÃ§Ãµes avanÃ§adas...")
        features_sem_multicolinearidade = remover_multicolinearidade(X_treino, features_completas, threshold=0.85)
        
        # 7. SELEÃ‡ÃƒO ESTATÃSTICA DE FEATURES  
        features_estatisticas = selecao_features_estatistica(
            X_treino[features_sem_multicolinearidade], y_treino, k=min(15, len(features_sem_multicolinearidade))
        )
        
        # 8. ENSEMBLE ROBUSTO
        ensemble_modelo = criar_ensemble_robusto()
        
        # 9. VALIDAÃ‡ÃƒO TEMPORAL ROBUSTA
        acuracia_ensemble, std_ensemble, r2_ensemble = validacao_temporal_robusta(
            X_treino, y_treino, y_treino_dir, ensemble_modelo, features_estatisticas
        )
        
        print(f"\nğŸ¯ MODELO ENSEMBLE OTIMIZADO:")
        print(f"   Features utilizadas: {len(features_estatisticas)}")
        print(f"   AcurÃ¡cia temporal: {acuracia_ensemble:.1%} (Â±{std_ensemble:.1%})")
        print(f"   RÂ² temporal: {r2_ensemble:.4f}")
        
        # 10. COMPARAÃ‡ÃƒO: Modelo simples vs Ensemble
        print(f"\nğŸ“Š COMPARAÃ‡ÃƒO DE ABORDAGENS:")
        melhores_features_orig, melhor_modelo_orig, melhor_acuracia_orig = testar_selecao_features(
            X_treino, y_treino, y_treino_dir, analise_correlacao
        )
        
        print(f"   ğŸ”¹ Abordagem Original: {melhor_acuracia_orig:.1%}")
        print(f"   ğŸ”¹ Ensemble Otimizado: {acuracia_ensemble:.1%}")
        
        # Escolher melhor abordagem
        if acuracia_ensemble > melhor_acuracia_orig:
            print(f"   âœ… Usando ENSEMBLE (melhor performance)")
            modelo_final = ensemble_modelo
            features_finais = features_estatisticas
            acuracia_cv_final = acuracia_ensemble
        else:
            print(f"   âœ… Usando abordagem ORIGINAL (melhor performance)")
            modelo_final = melhor_modelo_orig
            features_finais = melhores_features_orig
            acuracia_cv_final = melhor_acuracia_orig
        
        # 11. Treinar modelo final com as melhores configuraÃ§Ãµes
        print(f"\nğŸ¯ TREINAMENTO FINAL COM CONFIGURAÃ‡ÃƒO OTIMIZADA")
        print(f"Features selecionadas: {len(features_finais)}")
        print(f"Modelo selecionado: {type(modelo_final).__name__}")
        print(f"AcurÃ¡cia CV esperada: {acuracia_cv_final:.1%}")
        
        # Preparar dados com features selecionadas
        X_treino_otimizado = X_treino[features_finais]
        X_teste_otimizado = X_teste[features_finais]
        
        # 12. Teste final nos Ãºltimos 30 dias
        if isinstance(modelo_final, VotingRegressor):
            modelo_nome_final = "Ensemble"
        else:
            modelo_nome_final = modelo_final
            
        resultado_final = teste_final_30_dias(
            X_treino_otimizado, X_teste_otimizado, y_treino, y_teste, y_teste_dir, modelo_nome_final
        )
        
        # RESUMO FINAL COM FOCO NA META
        print("\n" + "="*70)
        print("RESUMO FINAL - AVALIAÃ‡ÃƒO DA META")
        print("="*70)
        
        print(f"ğŸ¯ META: 75% de acurÃ¡cia nos Ãºltimos 30 dias")
        print(f"ğŸ“Š RESULTADO: {resultado_final['acuracia']:.1%}")
        print(f"ğŸ† MODELO USADO: {resultado_final['modelo']}")
        print(f"ğŸ“ˆ STATUS: {resultado_final['status']}")
        print(f"ğŸ”§ FEATURES USADAS: {len(features_finais)} selecionadas")
        
        # ComparaÃ§Ã£o com validaÃ§Ã£o cruzada
        diferenca = resultado_final['acuracia'] - acuracia_cv_final
        print(f"\nğŸ” ANÃLISE DE CONSISTÃŠNCIA:")
        print(f"  ValidaÃ§Ã£o Cruzada: {acuracia_cv_final:.1%}")
        print(f"  Teste Final: {resultado_final['acuracia']:.1%}")
        print(f"  DiferenÃ§a: {diferenca:+.1%}")
        
        if abs(diferenca) < 0.05:
            print("âœ… Modelo consistente entre CV e teste final")
        else:
            print("âš ï¸ Grande diferenÃ§a entre CV e teste - verificar overfitting")
        
        # Baseline comparison
        baseline = max(y_teste_dir.mean(), 1 - y_teste_dir.mean())
        melhoria = resultado_final['acuracia'] - baseline
        print(f"\nğŸ“Š COMPARAÃ‡ÃƒO COM BASELINE:")
        print(f"  Baseline (classe majoritÃ¡ria): {baseline:.1%}")
        print(f"  Nosso modelo: {resultado_final['acuracia']:.1%}")
        print(f"  Melhoria: {melhoria*100:+.1f} pontos percentuais")
        
        # AnÃ¡lise da regressÃ£o
        print(f"\nğŸ“ˆ QUALIDADE DA REGRESSÃƒO:")
        print(f"  RÂ² no teste: {resultado_final['r2']:.4f}")
        print(f"  MSE no teste: {resultado_final['mse']:.6f}")
        print(f"  CorrelaÃ§Ã£o retorno real vs previsto: {np.corrcoef(resultado_final['real_retorno'], resultado_final['predicoes_retorno'])[0,1]:.4f}")
        
        # AnÃ¡lise das features selecionadas
        print(f"\nğŸ¯ FEATURES FINAIS SELECIONADAS:")
        for i, feature in enumerate(features_finais[:10], 1):  # Top 10
            corr_ret = analise_correlacao['correlacoes_retorno'].get(feature, 0)
            corr_dir = analise_correlacao['correlacoes_direcao'].get(feature, 0)
            print(f"  {i:2d}. {feature:<20}: ret={corr_ret:.4f}, dir={corr_dir:.4f}")
        
        if len(features_finais) > 10:
            print(f"     ... e mais {len(features_finais) - 10} features")
        
        if resultado_final['status'] == 'SUCESSO':
            print("\nğŸ‰ MISSÃƒO CUMPRIDA!")
            print("   Modelo atingiu a meta de 75% de acurÃ¡cia!")
            print("   A anÃ¡lise de correlaÃ§Ã£o foi fundamental para o sucesso!")
        else:
            print("\nğŸ“Š META NÃƒO ATINGIDA")
            print(f"   Melhor resultado alcanÃ§ado: {resultado_final['acuracia']:.1%}")
            print(f"   DistÃ¢ncia da meta: {(0.75 - resultado_final['acuracia'])*100:.1f} pontos")
            print("\nğŸ’¡ EstratÃ©gias adicionais para considerar:")
            print("   - Features de sentimento de mercado")
            print("   - Dados macroeconÃ´micos (taxa SELIC, cÃ¢mbio)")
            print("   - Modelos nÃ£o-lineares (SVM, Random Forest)")
            print("   - Dados de maior frequÃªncia (intraday)")
            print("   - Ensemble de mÃºltiplos modelos")
        
        print("\nâœ“ PROJETO CONCLUÃDO!")
        
        return {
            'teste_final': resultado_final,
            'features_selecionadas': features_finais,
            'modelo_selecionado': type(modelo_final).__name__,
            'analise_correlacao': analise_correlacao,
            'meta_atingida': resultado_final['status'] == 'SUCESSO',
            'acuracia_cv': acuracia_cv_final
        }
        
    except Exception as e:
        print(f"Erro: {e}")
        import traceback
        traceback.print_exc()
        return None


def testar_selecao_features(X_treino, y_treino, y_treino_dir, analise_correlacao):
    """
    TESTE DE SELEÃ‡ÃƒO DE FEATURES
    Testa diferentes conjuntos de features para maximizar a acurÃ¡cia
    """
    print("\n" + "="*60)
    print("TESTE DE SELEÃ‡ÃƒO DE FEATURES")
    print("="*60)
    
    # Diferentes conjuntos de features para testar
    conjuntos_features = {
        'TOP_5': analise_correlacao['top_5_features'],
        'TOP_8': analise_correlacao['top_8_features'],
        'TODAS_ORIGINAIS': [col for col in X_treino.columns if col in [
            'MM5', 'MM20', 'RSI', 'Volatilidade', 'MM_Cross', 'Price_Above_MA20', 
            'RSI_Signal', 'Returns_3d', 'Returns_7d', 'Volume_Spike', 'Low_Vol', 'Channel_Position'
        ]],
        'TODAS_DISPONIVEIS': list(X_treino.columns)
    }
    
    # Modelos de regressÃ£o para testar
    modelos = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0, random_state=42),
        'Lasso Regression': Lasso(alpha=0.1, random_state=42),
        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
    }
    
    # FunÃ§Ã£o personalizada para calcular acurÃ¡cia de direÃ§Ã£o
    def acuracia_direcao_regressao(y_true, y_pred):
        direcao_real = (y_true > 0).astype(int)
        direcao_pred = (y_pred > 0).astype(int)
        return np.mean(direcao_real == direcao_pred)
    
    scorer_acuracia = make_scorer(acuracia_direcao_regressao)
    
    resultados_completos = {}
    
    print("ğŸ§ª Testando diferentes combinaÃ§Ãµes de features e modelos...")
    
    # Testar cada conjunto de features
    for nome_conjunto, features_selecionadas in conjuntos_features.items():
        print(f"\nğŸ“Š Testando conjunto: {nome_conjunto} ({len(features_selecionadas)} features)")
        
        # Verificar se todas as features existem
        features_disponiveis = [f for f in features_selecionadas if f in X_treino.columns]
        if len(features_disponiveis) != len(features_selecionadas):
            print(f"   âš ï¸ Algumas features nÃ£o disponÃ­veis. Usando {len(features_disponiveis)} de {len(features_selecionadas)}")
        
        if len(features_disponiveis) == 0:
            print(f"   âŒ Nenhuma feature disponÃ­vel para {nome_conjunto}")
            continue
            
        X_treino_subset = X_treino[features_disponiveis]
        
        resultados_conjunto = {}
        
        # Testar cada modelo
        for nome_modelo, modelo_base in modelos.items():
            try:
                # Criar pipeline
                pipeline = Pipeline([
                    ('scaler', StandardScaler()),
                    ('regressor', modelo_base)
                ])
                
                # ValidaÃ§Ã£o cruzada
                cv = KFold(n_splits=5, shuffle=False)
                scores_acuracia = cross_val_score(
                    pipeline, X_treino_subset, y_treino, cv=cv, 
                    scoring=scorer_acuracia, n_jobs=-1
                )
                
                scores_r2 = cross_val_score(
                    pipeline, X_treino_subset, y_treino, cv=cv, 
                    scoring='r2', n_jobs=-1
                )
                
                # Armazenar resultados
                acuracia_media = scores_acuracia.mean()
                resultados_conjunto[nome_modelo] = {
                    'acuracia_mean': acuracia_media,
                    'acuracia_std': scores_acuracia.std(),
                    'r2_mean': scores_r2.mean(),
                    'r2_std': scores_r2.std()
                }
                
                # Mostrar resultado
                status = "âœ…" if acuracia_media >= 0.75 else "âš ï¸" if acuracia_media >= 0.60 else "âŒ"
                print(f"     {status} {nome_modelo:<15}: {acuracia_media:.1%} (Â±{scores_acuracia.std():.1%})")
                
            except Exception as e:
                print(f"     âŒ {nome_modelo:<15}: Erro - {str(e)[:50]}")
                continue
        
        # Encontrar melhor modelo para este conjunto
        if resultados_conjunto:
            melhor_modelo_conjunto = max(resultados_conjunto.keys(), 
                                       key=lambda k: resultados_conjunto[k]['acuracia_mean'])
            melhor_acuracia_conjunto = resultados_conjunto[melhor_modelo_conjunto]['acuracia_mean']
            
            resultados_completos[nome_conjunto] = {
                'features': features_disponiveis,
                'n_features': len(features_disponiveis),
                'melhor_modelo': melhor_modelo_conjunto,
                'melhor_acuracia': melhor_acuracia_conjunto,
                'resultados_modelos': resultados_conjunto
            }
            
            print(f"   ğŸ† Melhor: {melhor_modelo_conjunto} ({melhor_acuracia_conjunto:.1%})")
    
    # RANKING GERAL
    print(f"\n" + "="*60)
    print("ğŸ† RANKING GERAL - MELHORES COMBINAÃ‡Ã•ES")
    print("="*60)
    
    # Ordenar por acurÃ¡cia
    ranking = sorted(resultados_completos.items(), 
                    key=lambda x: x[1]['melhor_acuracia'], reverse=True)
    
    for i, (nome_conjunto, resultado) in enumerate(ranking, 1):
        acuracia = resultado['melhor_acuracia']
        modelo = resultado['melhor_modelo']
        n_features = resultado['n_features']
        
        status = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ”¸"
        meta_status = "âœ…" if acuracia >= 0.75 else "âš ï¸" if acuracia >= 0.60 else "âŒ"
        
        print(f"{status} {i}. {nome_conjunto:<18}: {acuracia:.1%} {meta_status} | {modelo} | {n_features} features")
    
    # MELHOR COMBINAÃ‡ÃƒO GERAL
    if ranking:
        melhor_geral = ranking[0]
        nome_melhor = melhor_geral[0]
        dados_melhor = melhor_geral[1]
        
        print(f"\nğŸ¯ MELHOR COMBINAÃ‡ÃƒO ENCONTRADA:")
        print(f"   Conjunto: {nome_melhor}")
        print(f"   Modelo: {dados_melhor['melhor_modelo']}")
        print(f"   AcurÃ¡cia: {dados_melhor['melhor_acuracia']:.1%}")
        print(f"   Features ({dados_melhor['n_features']}): {', '.join(dados_melhor['features'][:5])}{'...' if len(dados_melhor['features']) > 5 else ''}")
        
        if dados_melhor['melhor_acuracia'] >= 0.75:
            print("   ğŸ‰ META DE 75% ATINGIDA!")
        else:
            print(f"   ğŸ“Š Faltam {(0.75 - dados_melhor['melhor_acuracia'])*100:.1f} pontos para a meta")
        
        return dados_melhor['features'], dados_melhor['melhor_modelo'], dados_melhor['melhor_acuracia']
    
    return None, None, 0.0

def remover_multicolinearidade(X_treino, features_lista, threshold=0.9):
    """
    Remove features com alta multicolinearidade
    """
    print(f"ğŸ”§ Removendo multicolinearidade (threshold: {threshold})...")
    
    # Calcular matriz de correlaÃ§Ã£o
    corr_matrix = X_treino[features_lista].corr().abs()
    
    # Encontrar features para remover
    features_to_remove = set()
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if corr_matrix.iloc[i, j] > threshold:
                # Remover a feature com menor correlaÃ§Ã£o com o target
                colname = corr_matrix.columns[j]
                features_to_remove.add(colname)
    
    # Features finais
    features_finais = [f for f in features_lista if f not in features_to_remove]
    
    print(f"   ğŸ“Š Features removidas: {len(features_to_remove)}")
    print(f"   âœ… Features mantidas: {len(features_finais)}")
    
    if len(features_to_remove) > 0:
        print(f"   ğŸ—‘ï¸ Removidas: {list(features_to_remove)[:5]}{'...' if len(features_to_remove) > 5 else ''}")
    
    return features_finais

def selecao_features_estatistica(X_treino, y_treino, k=10):
    """
    SeleÃ§Ã£o de features usando mÃ©todos estatÃ­sticos
    """
    print(f"ğŸ“Š SeleÃ§Ã£o estatÃ­stica de features (k={k})...")
    
    # SelectKBest com f_regression
    selector = SelectKBest(score_func=f_regression, k=k)
    X_selected = selector.fit_transform(X_treino, y_treino)
    
    # Obter features selecionadas
    feature_mask = selector.get_support()
    features_selecionadas = X_treino.columns[feature_mask].tolist()
    
    # Mostrar scores
    scores = selector.scores_
    feature_scores = list(zip(X_treino.columns, scores))
    feature_scores.sort(key=lambda x: x[1], reverse=True)
    
    print(f"   âœ… {len(features_selecionadas)} features selecionadas")
    print(f"   ğŸ† Top 5 scores:")
    for i, (feature, score) in enumerate(feature_scores[:5], 1):
        print(f"     {i}. {feature:<20}: {score:.2f}")
    
    return features_selecionadas

def criar_ensemble_robusto():
    """
    Cria ensemble de modelos para maior robustez
    """
    print("ğŸ¤– Criando ensemble de modelos...")
    
    # Modelos com diferentes caracterÃ­sticas
    modelos = [
        ('linear', LinearRegression()),
        ('ridge', Ridge(alpha=1.0, random_state=42)),
        ('lasso', Lasso(alpha=0.01, random_state=42)),
        ('elastic', ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42))
    ]
    
    # Ensemble com votaÃ§Ã£o
    ensemble = VotingRegressor(estimators=modelos)
    
    print(f"   âœ… Ensemble criado com {len(modelos)} modelos")
    
    return ensemble

def validacao_temporal_robusta(X_treino, y_treino, y_treino_dir, modelo, features_selecionadas):
    """
    ValidaÃ§Ã£o cruzada temporal mais robusta
    """
    print("â° ValidaÃ§Ã£o cruzada temporal robusta...")
    
    # Usar apenas features selecionadas
    X_treino_sel = X_treino[features_selecionadas]
    
    # TimeSeriesSplit para dados temporais
    tscv = TimeSeriesSplit(n_splits=5)
    
    # Pipeline com ensemble
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', modelo)
    ])
    
    # FunÃ§Ã£o para calcular acurÃ¡cia de direÃ§Ã£o
    def acuracia_direcao_regressao(y_true, y_pred):
        direcao_real = (y_true > 0).astype(int)
        direcao_pred = (y_pred > 0).astype(int)
        return np.mean(direcao_real == direcao_pred)
    
    scorer_acuracia = make_scorer(acuracia_direcao_regressao)
    
    # ValidaÃ§Ã£o cruzada temporal
    scores_acuracia = cross_val_score(
        pipeline, X_treino_sel, y_treino, cv=tscv, 
        scoring=scorer_acuracia, n_jobs=-1
    )
    
    scores_r2 = cross_val_score(
        pipeline, X_treino_sel, y_treino, cv=tscv, 
        scoring='r2', n_jobs=-1
    )
    
    acuracia_media = scores_acuracia.mean()
    acuracia_std = scores_acuracia.std()
    r2_medio = scores_r2.mean()
    
    print(f"   ğŸ“Š AcurÃ¡cia temporal: {acuracia_media:.1%} (Â±{acuracia_std:.1%})")
    print(f"   ğŸ“ˆ RÂ² temporal: {r2_medio:.4f}")
    
    return acuracia_media, acuracia_std, r2_medio
