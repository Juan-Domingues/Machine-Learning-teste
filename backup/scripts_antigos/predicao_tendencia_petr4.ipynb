{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c38b73a",
   "metadata": {},
   "source": [
    "# Previs√£o de Tend√™ncia da PETR4.SA usando Machine Learning\n",
    "\n",
    "## Objetivo\n",
    "Desenvolver um modelo de machine learning para prever a tend√™ncia (‚Üë ou ‚Üì) de uma s√©rie temporal financeira da PETR4.SA, com acur√°cia m√≠nima de 75% no conjunto de teste (√∫ltimos 30 dias).\n",
    "\n",
    "## Outline do Projeto\n",
    "1. **Importa√ß√£o de Bibliotecas**\n",
    "2. **Carregamento e Explora√ß√£o dos Dados**\n",
    "3. **Pr√©-processamento dos Dados**\n",
    "4. **Engenharia de Features**\n",
    "5. **Divis√£o Treino/Teste**\n",
    "6. **Treinamento de M√∫ltiplos Modelos**\n",
    "7. **Avalia√ß√£o dos Modelos**\n",
    "8. **Previs√µes e Visualiza√ß√£o dos Resultados**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f3c44",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas para manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Bibliotecas para visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Bibliotecas para download de dados financeiros\n",
    "import yfinance as yf\n",
    "\n",
    "# Bibliotecas para machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Bibliotecas para an√°lise t√©cnica\n",
    "import talib\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537aea0",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Explora√ß√£o dos Dados\n",
    "\n",
    "Vamos carregar os dados hist√≥ricos da PETR4.SA usando a biblioteca yfinance e explorar a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o per√≠odo de an√°lise (2 anos de dados hist√≥ricos)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=730)  # ~2 anos\n",
    "\n",
    "# Carregar dados da PETR4.SA\n",
    "ticker = \"PETR4.SA\"\n",
    "print(f\"Carregando dados de {ticker} de {start_date.strftime('%Y-%m-%d')} at√© {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "print(f\"\\nüìä Dados carregados: {len(data)} registros\")\n",
    "print(f\"Per√≠odo: {data.index[0].strftime('%Y-%m-%d')} a {data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Visualizar primeiras e √∫ltimas linhas\n",
    "print(\"\\nüîç Primeiras 5 linhas:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\nüîç √öltimas 5 linhas:\")\n",
    "display(data.tail())\n",
    "\n",
    "# Informa√ß√µes b√°sicas sobre os dados\n",
    "print(\"\\nüìà Informa√ß√µes dos dados:\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nüìä Estat√≠sticas descritivas:\")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b14aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores ausentes\n",
    "print(\"üîç Verificando valores ausentes:\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Encontrados valores ausentes. Ser√° necess√°rio tratamento.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhum valor ausente encontrado!\")\n",
    "\n",
    "# Visualiza√ß√£o inicial dos pre√ßos\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=('Pre√ßo de Fechamento', 'Volume', 'High-Low', 'Open-Close'),\n",
    "                    vertical_spacing=0.08)\n",
    "\n",
    "# Pre√ßo de fechamento\n",
    "fig.add_trace(go.Scatter(x=data.index, y=data['Close'], name='Close', line=dict(color='blue')), row=1, col=1)\n",
    "\n",
    "# Volume\n",
    "fig.add_trace(go.Scatter(x=data.index, y=data['Volume'], name='Volume', line=dict(color='orange')), row=1, col=2)\n",
    "\n",
    "# High-Low\n",
    "fig.add_trace(go.Scatter(x=data.index, y=data['High'] - data['Low'], name='High-Low', line=dict(color='green')), row=2, col=1)\n",
    "\n",
    "# Open-Close\n",
    "fig.add_trace(go.Scatter(x=data.index, y=data['Close'] - data['Open'], name='Close-Open', line=dict(color='red')), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"üìà An√°lise Explorat√≥ria - PETR4.SA\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378fda8",
   "metadata": {},
   "source": [
    "## 3. Pr√©-processamento dos Dados\n",
    "\n",
    "Nesta se√ß√£o, vamos limpar e preparar os dados para an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ab92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma c√≥pia dos dados para processamento\n",
    "df = data.copy()\n",
    "\n",
    "# Renomear colunas para facilitar o uso\n",
    "df.columns = ['open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "\n",
    "# Remover dados ausentes (se houver)\n",
    "df = df.dropna()\n",
    "\n",
    "# Criar vari√°vel target: 1 se o pre√ßo subiu, 0 se desceu\n",
    "df['price_change'] = df['close'].pct_change()\n",
    "df['target'] = (df['price_change'] > 0).astype(int)\n",
    "\n",
    "# Remover a primeira linha (NaN devido ao pct_change)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"üìä Dados ap√≥s pr√©-processamento: {len(df)} registros\")\n",
    "print(f\"\\nüéØ Distribui√ß√£o da vari√°vel target:\")\n",
    "target_dist = df['target'].value_counts()\n",
    "print(f\"Descida (0): {target_dist[0]} ({target_dist[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Subida (1): {target_dist[1]} ({target_dist[1]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualizar distribui√ß√£o do target\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "target_dist.plot(kind='bar', ax=ax1, color=['red', 'green'])\n",
    "ax1.set_title('Distribui√ß√£o da Tend√™ncia')\n",
    "ax1.set_xlabel('Tend√™ncia (0=Descida, 1=Subida)')\n",
    "ax1.set_ylabel('Frequ√™ncia')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# S√©rie temporal da tend√™ncia\n",
    "ax2.plot(df.index, df['target'], alpha=0.7, color='purple')\n",
    "ax2.set_title('Tend√™ncia ao Longo do Tempo')\n",
    "ax2.set_xlabel('Data')\n",
    "ax2.set_ylabel('Tend√™ncia (0=Descida, 1=Subida)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de23e3",
   "metadata": {},
   "source": [
    "## 4. Engenharia de Features\n",
    "\n",
    "Vamos criar indicadores t√©cnicos e features que podem ajudar na previs√£o da tend√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97717323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_features(df):\n",
    "    \"\"\"\n",
    "    Cria features t√©cnicas para an√°lise de s√©ries temporais financeiras\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # ========== FEATURES B√ÅSICAS ==========\n",
    "    # Retornos percentuais\n",
    "    for period in [1, 2, 3, 5, 10]:\n",
    "        df_features[f'return_{period}d'] = df_features['close'].pct_change(period)\n",
    "    \n",
    "    # Volatilidade (desvio padr√£o dos retornos)\n",
    "    for period in [5, 10, 20]:\n",
    "        df_features[f'volatility_{period}d'] = df_features['price_change'].rolling(period).std()\n",
    "    \n",
    "    # ========== M√âDIAS M√ìVEIS ==========\n",
    "    for period in [5, 10, 20, 50]:\n",
    "        df_features[f'sma_{period}'] = df_features['close'].rolling(period).mean()\n",
    "        df_features[f'price_sma_{period}_ratio'] = df_features['close'] / df_features[f'sma_{period}']\n",
    "    \n",
    "    # M√©dias m√≥veis exponenciais\n",
    "    for period in [12, 26]:\n",
    "        df_features[f'ema_{period}'] = df_features['close'].ewm(span=period).mean()\n",
    "    \n",
    "    # ========== INDICADORES T√âCNICOS COM TALIB ==========\n",
    "    try:\n",
    "        # RSI (Relative Strength Index)\n",
    "        df_features['rsi_14'] = talib.RSI(df_features['close'].values, timeperiod=14)\n",
    "        \n",
    "        # MACD\n",
    "        macd, macd_signal, macd_hist = talib.MACD(df_features['close'].values)\n",
    "        df_features['macd'] = macd\n",
    "        df_features['macd_signal'] = macd_signal\n",
    "        df_features['macd_histogram'] = macd_hist\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower = talib.BBANDS(df_features['close'].values)\n",
    "        df_features['bb_upper'] = bb_upper\n",
    "        df_features['bb_lower'] = bb_lower\n",
    "        df_features['bb_position'] = (df_features['close'] - bb_lower) / (bb_upper - bb_lower)\n",
    "        \n",
    "        # Stochastic Oscillator\n",
    "        stoch_k, stoch_d = talib.STOCH(df_features['high'].values, df_features['low'].values, df_features['close'].values)\n",
    "        df_features['stoch_k'] = stoch_k\n",
    "        df_features['stoch_d'] = stoch_d\n",
    "        \n",
    "        # Williams %R\n",
    "        df_features['williams_r'] = talib.WILLR(df_features['high'].values, df_features['low'].values, df_features['close'].values)\n",
    "        \n",
    "        print(\"‚úÖ Indicadores t√©cnicos (TA-Lib) criados com sucesso!\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è TA-Lib n√£o dispon√≠vel. Criando indicadores manualmente...\")\n",
    "        \n",
    "        # RSI manual\n",
    "        delta = df_features['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df_features['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD manual\n",
    "        ema_12 = df_features['close'].ewm(span=12).mean()\n",
    "        ema_26 = df_features['close'].ewm(span=26).mean()\n",
    "        df_features['macd'] = ema_12 - ema_26\n",
    "        df_features['macd_signal'] = df_features['macd'].ewm(span=9).mean()\n",
    "        df_features['macd_histogram'] = df_features['macd'] - df_features['macd_signal']\n",
    "    \n",
    "    # ========== FEATURES DE VOLUME ==========\n",
    "    df_features['volume_sma_10'] = df_features['volume'].rolling(10).mean()\n",
    "    df_features['volume_ratio'] = df_features['volume'] / df_features['volume_sma_10']\n",
    "    \n",
    "    # ========== FEATURES DE PADR√ïES DE PRE√áO ==========\n",
    "    # High-Low range\n",
    "    df_features['hl_range'] = (df_features['high'] - df_features['low']) / df_features['close']\n",
    "    \n",
    "    # Open-Close range\n",
    "    df_features['oc_range'] = (df_features['close'] - df_features['open']) / df_features['open']\n",
    "    \n",
    "    # Posi√ß√£o do fechamento no range do dia\n",
    "    df_features['close_position'] = (df_features['close'] - df_features['low']) / (df_features['high'] - df_features['low'])\n",
    "    \n",
    "    # ========== FEATURES TEMPORAIS ==========\n",
    "    df_features['day_of_week'] = df_features.index.dayofweek\n",
    "    df_features['month'] = df_features.index.month\n",
    "    df_features['quarter'] = df_features.index.quarter\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Criar features\n",
    "print(\"üî® Criando features t√©cnicas...\")\n",
    "df_with_features = create_technical_features(df)\n",
    "\n",
    "print(f\"‚úÖ Features criadas! Total de colunas: {len(df_with_features.columns)}\")\n",
    "print(f\"üìä Shape dos dados: {df_with_features.shape}\")\n",
    "\n",
    "# Remover linhas com NaN (criados pelos indicadores)\n",
    "df_clean = df_with_features.dropna()\n",
    "print(f\"üìä Dados limpos: {len(df_clean)} registros\")\n",
    "\n",
    "# Mostrar algumas features criadas\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['open', 'high', 'low', 'close', 'adj_close', 'volume', 'price_change', 'target']]\n",
    "print(f\"\\nüéØ Features criadas ({len(feature_columns)}):\")\n",
    "for i, feat in enumerate(feature_columns[:10]):\n",
    "    print(f\"{i+1:2d}. {feat}\")\n",
    "if len(feature_columns) > 10:\n",
    "    print(f\"    ... e mais {len(feature_columns) - 10} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar correla√ß√£o das features com o target\n",
    "correlations = df_clean[feature_columns + ['target']].corr()['target'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"üéØ Top 15 features mais correlacionadas com o target:\")\n",
    "print(correlations.head(16)[1:])  # Excluir o pr√≥prio target\n",
    "\n",
    "# Visualizar correla√ß√µes\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = correlations.head(16)[1:15]  # Top 15 excluindo target\n",
    "plt.barh(range(len(top_features)), top_features.values)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Correla√ß√£o Absoluta com Target')\n",
    "plt.title('Top 15 Features por Correla√ß√£o com Target')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selecionar features para o modelo\n",
    "selected_features = correlations.head(21)[1:].index.tolist()  # Top 20 features\n",
    "print(f\"\\n‚úÖ Selecionadas {len(selected_features)} features para o modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2155480",
   "metadata": {},
   "source": [
    "## 5. Divis√£o Treino/Teste\n",
    "\n",
    "Vamos dividir os dados considerando que os √∫ltimos 30 dias ser√£o nosso conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para machine learning\n",
    "X = df_clean[selected_features]\n",
    "y = df_clean['target']\n",
    "\n",
    "print(f\"üìä Prepara√ß√£o dos dados:\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "\n",
    "# Divis√£o temporal: √∫ltimos 30 dias para teste\n",
    "split_date = df_clean.index[-30]  # 30 dias antes do final\n",
    "print(f\"\\nüìÖ Data de divis√£o: {split_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Conjuntos de treino e teste\n",
    "X_train = X[X.index < split_date]\n",
    "X_test = X[X.index >= split_date]\n",
    "y_train = y[y.index < split_date]\n",
    "y_test = y[y.index >= split_date]\n",
    "\n",
    "print(f\"\\nüìä Divis√£o dos dados:\")\n",
    "print(f\"Treino: {len(X_train)} registros ({X_train.index[0].strftime('%Y-%m-%d')} a {X_train.index[-1].strftime('%Y-%m-%d')})\")\n",
    "print(f\"Teste:  {len(X_test)} registros ({X_test.index[0].strftime('%Y-%m-%d')} a {X_test.index[-1].strftime('%Y-%m-%d')})\")\n",
    "\n",
    "# Verificar distribui√ß√£o do target em cada conjunto\n",
    "print(f\"\\nüéØ Distribui√ß√£o do target:\")\n",
    "print(f\"Treino - Descida: {(y_train==0).sum()} ({(y_train==0).mean()*100:.1f}%), Subida: {(y_train==1).sum()} ({(y_train==1).mean()*100:.1f}%)\")\n",
    "print(f\"Teste  - Descida: {(y_test==0).sum()} ({(y_test==0).mean()*100:.1f}%), Subida: {(y_test==1).sum()} ({(y_test==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Visualizar a divis√£o\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Pre√ßos com divis√£o\n",
    "ax1.plot(df_clean.index, df_clean['close'], label='Pre√ßo de Fechamento', alpha=0.8)\n",
    "ax1.axvline(x=split_date, color='red', linestyle='--', alpha=0.8, label=f'Divis√£o Treino/Teste')\n",
    "ax1.fill_between(X_train.index, df_clean.loc[X_train.index, 'close'].min(), \n",
    "                df_clean.loc[X_train.index, 'close'].max(), alpha=0.2, color='blue', label='Treino')\n",
    "ax1.fill_between(X_test.index, df_clean.loc[X_test.index, 'close'].min(), \n",
    "                df_clean.loc[X_test.index, 'close'].max(), alpha=0.2, color='red', label='Teste')\n",
    "ax1.set_title('Divis√£o Treino/Teste - Pre√ßos PETR4.SA')\n",
    "ax1.set_ylabel('Pre√ßo (R$)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Target com divis√£o\n",
    "ax2.plot(df_clean.index, df_clean['target'], alpha=0.7, color='purple', label='Target')\n",
    "ax2.axvline(x=split_date, color='red', linestyle='--', alpha=0.8, label='Divis√£o Treino/Teste')\n",
    "ax2.fill_between(X_train.index, -0.1, 1.1, alpha=0.2, color='blue', label='Treino')\n",
    "ax2.fill_between(X_test.index, -0.1, 1.1, alpha=0.2, color='red', label='Teste')\n",
    "ax2.set_title('Target (Tend√™ncia) - Divis√£o Treino/Teste')\n",
    "ax2.set_ylabel('Tend√™ncia (0=Descida, 1=Subida)')\n",
    "ax2.set_xlabel('Data')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a97ab",
   "metadata": {},
   "source": [
    "## 6. Treinamento de M√∫ltiplos Modelos\n",
    "\n",
    "Vamos treinar diferentes algoritmos de machine learning e compar√°-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar pipelines com normaliza√ß√£o\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    \n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "    ]),\n",
    "    \n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'SVM': Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('classifier', SVC(probability=True, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Treinar e avaliar cada modelo\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"üöÄ Iniciando treinamento dos modelos...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"üìö Treinando {name}...\")\n",
    "    \n",
    "    # Treinamento\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Previs√µes\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation no conjunto de treino\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred_test,\n",
    "        'probabilities': y_pred_proba_test\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"   ‚úÖ Acur√°cia Treino: {train_acc:.4f}\")\n",
    "    print(f\"   ‚úÖ Acur√°cia Teste:  {test_acc:.4f}\")\n",
    "    print(f\"   ‚úÖ CV (m√©dia¬±std):  {cv_scores.mean():.4f}¬±{cv_scores.std():.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"üéâ Treinamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar ensemble (Voting Classifier) com os melhores modelos\n",
    "print(\"ü§ù Criando modelo ensemble...\")\n",
    "\n",
    "# Selecionar modelos base para o ensemble\n",
    "ensemble_models = [\n",
    "    ('rf', trained_models['Random Forest']),\n",
    "    ('gb', trained_models['Gradient Boosting']),\n",
    "    ('lr', trained_models['Logistic Regression'])\n",
    "]\n",
    "\n",
    "ensemble = VotingClassifier(estimators=ensemble_models, voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar ensemble\n",
    "y_pred_ensemble_train = ensemble.predict(X_train)\n",
    "y_pred_ensemble_test = ensemble.predict(X_test)\n",
    "y_pred_ensemble_proba = ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "ensemble_train_acc = accuracy_score(y_train, y_pred_ensemble_train)\n",
    "ensemble_test_acc = accuracy_score(y_test, y_pred_ensemble_test)\n",
    "ensemble_cv_scores = cross_val_score(ensemble, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "results['Ensemble'] = {\n",
    "    'train_accuracy': ensemble_train_acc,\n",
    "    'test_accuracy': ensemble_test_acc,\n",
    "    'cv_mean': ensemble_cv_scores.mean(),\n",
    "    'cv_std': ensemble_cv_scores.std(),\n",
    "    'predictions': y_pred_ensemble_test,\n",
    "    'probabilities': y_pred_ensemble_proba\n",
    "}\n",
    "\n",
    "trained_models['Ensemble'] = ensemble\n",
    "\n",
    "print(f\"‚úÖ Ensemble - Acur√°cia Treino: {ensemble_train_acc:.4f}\")\n",
    "print(f\"‚úÖ Ensemble - Acur√°cia Teste:  {ensemble_test_acc:.4f}\")\n",
    "print(f\"‚úÖ Ensemble - CV (m√©dia¬±std):  {ensemble_cv_scores.mean():.4f}¬±{ensemble_cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8d35e",
   "metadata": {},
   "source": [
    "## 7. Avalia√ß√£o dos Modelos\n",
    "\n",
    "Vamos comparar o desempenho de todos os modelos e identificar o melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': list(results.keys()),\n",
    "    'Acur√°cia Treino': [results[model]['train_accuracy'] for model in results.keys()],\n",
    "    'Acur√°cia Teste': [results[model]['test_accuracy'] for model in results.keys()],\n",
    "    'CV M√©dia': [results[model]['cv_mean'] for model in results.keys()],\n",
    "    'CV Desvio': [results[model]['cv_std'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "# Ordenar por acur√°cia no teste\n",
    "results_df = results_df.sort_values('Acur√°cia Teste', ascending=False)\n",
    "\n",
    "print(\"üìä RESULTADOS COMPARATIVOS DOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Identificar o melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_accuracy = results_df.iloc[0]['Acur√°cia Teste']\n",
    "\n",
    "print(f\"\\nüèÜ MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"üìà Acur√°cia no teste: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Verificar se atende o crit√©rio de 75%\n",
    "if best_accuracy >= 0.75:\n",
    "    print(f\"‚úÖ OBJETIVO ATINGIDO! Acur√°cia ‚â• 75%\")\n",
    "else:\n",
    "    print(f\"‚ùå Objetivo n√£o atingido. Acur√°cia atual: {best_accuracy*100:.2f}% (meta: 75%)\")\n",
    "    print(\"üí° Sugest√µes para melhoria:\")\n",
    "    print(\"   - Adicionar mais features\")\n",
    "    print(\"   - Otimizar hiperpar√¢metros\")\n",
    "    print(\"   - Usar mais dados hist√≥ricos\")\n",
    "    print(\"   - Aplicar t√©cnicas de feature selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e95522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o comparativa dos modelos\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Compara√ß√£o de acur√°cias\n",
    "x_pos = np.arange(len(results_df))\n",
    "ax1.bar(x_pos - 0.2, results_df['Acur√°cia Treino'], 0.4, label='Treino', alpha=0.8)\n",
    "ax1.bar(x_pos + 0.2, results_df['Acur√°cia Teste'], 0.4, label='Teste', alpha=0.8)\n",
    "ax1.axhline(y=0.75, color='red', linestyle='--', alpha=0.7, label='Meta 75%')\n",
    "ax1.set_xlabel('Modelos')\n",
    "ax1.set_ylabel('Acur√°cia')\n",
    "ax1.set_title('Compara√ß√£o de Acur√°cias')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(results_df['Modelo'], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Cross-validation scores\n",
    "ax2.errorbar(x_pos, results_df['CV M√©dia'], yerr=results_df['CV Desvio'], \n",
    "            fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "ax2.axhline(y=0.75, color='red', linestyle='--', alpha=0.7, label='Meta 75%')\n",
    "ax2.set_xlabel('Modelos')\n",
    "ax2.set_ylabel('Acur√°cia CV')\n",
    "ax2.set_title('Cross-Validation (5-fold)')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(results_df['Modelo'], rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Matriz de confus√£o do melhor modelo\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax3, cmap='Blues',\n",
    "           xticklabels=['Descida', 'Subida'], yticklabels=['Descida', 'Subida'])\n",
    "ax3.set_title(f'Matriz de Confus√£o - {best_model_name}')\n",
    "ax3.set_xlabel('Predito')\n",
    "ax3.set_ylabel('Real')\n",
    "\n",
    "# 4. Distribui√ß√£o de probabilidades\n",
    "best_probabilities = results[best_model_name]['probabilities']\n",
    "ax4.hist(best_probabilities[y_test == 0], bins=20, alpha=0.7, label='Descida Real', color='red')\n",
    "ax4.hist(best_probabilities[y_test == 1], bins=20, alpha=0.7, label='Subida Real', color='green')\n",
    "ax4.axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Threshold 0.5')\n",
    "ax4.set_xlabel('Probabilidade Predita')\n",
    "ax4.set_ylabel('Frequ√™ncia')\n",
    "ax4.set_title(f'Distribui√ß√£o de Probabilidades - {best_model_name}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relat√≥rio detalhado do melhor modelo\n",
    "print(f\"\\nüìã RELAT√ìRIO DETALHADO - {best_model_name}\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, best_predictions, target_names=['Descida', 'Subida']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4db93",
   "metadata": {},
   "source": [
    "## 8. Previs√µes e Visualiza√ß√£o dos Resultados\n",
    "\n",
    "Vamos visualizar as previs√µes do melhor modelo e analisar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf27c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para visualiza√ß√£o\n",
    "test_dates = X_test.index\n",
    "test_prices = df_clean.loc[test_dates, 'close']\n",
    "actual_trends = y_test.values\n",
    "predicted_trends = results[best_model_name]['predictions']\n",
    "predicted_probabilities = results[best_model_name]['probabilities']\n",
    "\n",
    "# Criar DataFrame para an√°lise\n",
    "analysis_df = pd.DataFrame({\n",
    "    'Data': test_dates,\n",
    "    'Pre√ßo': test_prices.values,\n",
    "    'Tend√™ncia_Real': actual_trends,\n",
    "    'Tend√™ncia_Predita': predicted_trends,\n",
    "    'Probabilidade': predicted_probabilities,\n",
    "    'Acerto': actual_trends == predicted_trends\n",
    "})\n",
    "\n",
    "print(f\"üìä AN√ÅLISE DAS PREVIS√ïES - √öltimos 30 dias\")\n",
    "print(f\"Per√≠odo: {test_dates[0].strftime('%Y-%m-%d')} a {test_dates[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"\\nüéØ Resultados:\")\n",
    "print(f\"Total de previs√µes: {len(analysis_df)}\")\n",
    "print(f\"Acertos: {analysis_df['Acerto'].sum()} ({analysis_df['Acerto'].mean()*100:.1f}%)\")\n",
    "print(f\"Erros: {(~analysis_df['Acerto']).sum()} ({(~analysis_df['Acerto']).mean()*100:.1f}%)\")\n",
    "\n",
    "# An√°lise por tipo de movimento\n",
    "subidas_reais = analysis_df[analysis_df['Tend√™ncia_Real'] == 1]\n",
    "descidas_reais = analysis_df[analysis_df['Tend√™ncia_Real'] == 0]\n",
    "\n",
    "print(f\"\\nüìà Subidas (Real):\")\n",
    "print(f\"Total: {len(subidas_reais)}\")\n",
    "print(f\"Acertos: {subidas_reais['Acerto'].sum()} ({subidas_reais['Acerto'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìâ Descidas (Real):\")\n",
    "print(f\"Total: {len(descidas_reais)}\")\n",
    "print(f\"Acertos: {descidas_reais['Acerto'].sum()} ({descidas_reais['Acerto'].mean()*100:.1f}%)\")\n",
    "\n",
    "display(analysis_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o abrangente dos resultados\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Pre√ßos e Previs√µes', 'Probabilidades de Previs√£o',\n",
    "        'Acertos vs Erros', 'Retornos Reais vs Preditos',\n",
    "        'Distribui√ß√£o de Acertos', 'Performance por Per√≠odo'\n",
    "    ),\n",
    "    specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# 1. Pre√ßos e Previs√µes\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=analysis_df['Data'], y=analysis_df['Pre√ßo'], \n",
    "              name='Pre√ßo PETR4', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adicionar marcadores para previs√µes\n",
    "acertos = analysis_df[analysis_df['Acerto']]\n",
    "erros = analysis_df[~analysis_df['Acerto']]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=acertos['Data'], y=acertos['Pre√ßo'], \n",
    "              mode='markers', name='Acertos', \n",
    "              marker=dict(color='green', size=8, symbol='circle')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=erros['Data'], y=erros['Pre√ßo'], \n",
    "              mode='markers', name='Erros', \n",
    "              marker=dict(color='red', size=8, symbol='x')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Probabilidades\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=analysis_df['Data'], y=analysis_df['Probabilidade'], \n",
    "              name='Prob. Subida', line=dict(color='purple')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"black\", row=1, col=2)\n",
    "\n",
    "# 3. Acertos vs Erros ao longo do tempo\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=analysis_df['Data'], y=analysis_df['Acerto'].astype(int), \n",
    "              mode='markers+lines', name='Acertos (1) vs Erros (0)',\n",
    "              line=dict(color='orange')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Retornos\n",
    "returns_real = np.where(analysis_df['Tend√™ncia_Real'] == 1, 1, -1)\n",
    "returns_pred = np.where(analysis_df['Tend√™ncia_Predita'] == 1, 1, -1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=analysis_df['Data'], y=returns_real, \n",
    "              name='Retorno Real', line=dict(color='blue')),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=analysis_df['Data'], y=returns_pred, \n",
    "              name='Retorno Predito', line=dict(color='red', dash='dash')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Distribui√ß√£o de acertos\n",
    "accuracy_by_week = analysis_df.groupby(analysis_df['Data'].dt.week)['Acerto'].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=accuracy_by_week.index, y=accuracy_by_week.values, \n",
    "          name='Acur√°cia Semanal'),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Performance acumulada\n",
    "cumulative_accuracy = analysis_df['Acerto'].expanding().mean()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=analysis_df['Data'], y=cumulative_accuracy, \n",
    "              name='Acur√°cia Acumulada', line=dict(color='green')),\n",
    "    row=3, col=2\n",
    ")\n",
    "fig.add_hline(y=0.75, line_dash=\"dash\", line_color=\"red\", row=3, col=2)\n",
    "\n",
    "# Configurar layout\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    title_text=f\"üìä An√°lise Completa - {best_model_name} (Acur√°cia: {best_accuracy:.2%})\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Configurar eixos\n",
    "fig.update_xaxes(title_text=\"Data\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"Data\", row=3, col=2)\n",
    "fig.update_yaxes(title_text=\"Pre√ßo (R$)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Probabilidade\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Acerto (1) / Erro (0)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Retorno\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Acur√°cia\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Acur√°cia Acumulada\", row=3, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28abff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de feature importance (para modelos tree-based)\n",
    "if 'Random Forest' in best_model_name or 'Gradient Boosting' in best_model_name:\n",
    "    # Extrair feature importances\n",
    "    if best_model_name == 'Ensemble':\n",
    "        # Para ensemble, usar Random Forest como refer√™ncia\n",
    "        feature_importance = trained_models['Random Forest']['classifier'].feature_importances_\n",
    "    else:\n",
    "        feature_importance = trained_models[best_model_name]['classifier'].feature_importances_\n",
    "    \n",
    "    # Criar DataFrame de import√¢ncias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"üéØ TOP 10 FEATURES MAIS IMPORTANTES:\")\n",
    "    display(importance_df.head(10))\n",
    "    \n",
    "    # Visualizar import√¢ncias\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Import√¢ncia')\n",
    "    plt.title(f'Top 15 Features - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Resumo final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ RESUMO FINAL DO PROJETO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Dados analisados: {len(df_clean)} registros\")\n",
    "print(f\"üéØ Per√≠odo de teste: {len(y_test)} dias (√∫ltimos 30 dias)\")\n",
    "print(f\"ü§ñ Melhor modelo: {best_model_name}\")\n",
    "print(f\"üìà Acur√°cia obtida: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"üéØ Meta de acur√°cia: 75%\")\n",
    "\n",
    "if best_accuracy >= 0.75:\n",
    "    print(f\"‚úÖ SUCESSO: Meta atingida!\")\n",
    "    print(f\"üí∞ O modelo pode ser usado para apoiar decis√µes de trading\")\n",
    "else:\n",
    "    print(f\"‚ùå Meta n√£o atingida (diferen√ßa: {(0.75 - best_accuracy)*100:.2f}%)\")\n",
    "    print(f\"üí° Recomenda√ß√µes para melhoria no futuro\")\n",
    "\n",
    "print(f\"\\nüìù Features utilizadas: {len(selected_features)}\")\n",
    "print(f\"‚è∞ Per√≠odo de an√°lise: {df_clean.index[0].strftime('%Y-%m-%d')} a {df_clean.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0236fc",
   "metadata": {},
   "source": [
    "## üìã Conclus√µes e Pr√≥ximos Passos\n",
    "\n",
    "### Principais Resultados\n",
    "- **Modelo desenvolvido**: Sistema de previs√£o de tend√™ncia para PETR4.SA\n",
    "- **Dados utilizados**: ~2 anos de dados hist√≥ricos\n",
    "- **Features criadas**: Indicadores t√©cnicos, m√©dias m√≥veis, volatilidade, etc.\n",
    "- **Modelos testados**: Logistic Regression, Random Forest, Gradient Boosting, SVM, Ensemble\n",
    "- **Per√≠odo de teste**: √öltimos 30 dias de dados\n",
    "\n",
    "### Metodologia Aplicada\n",
    "1. ‚úÖ **Coleta de dados**: yfinance para dados hist√≥ricos da PETR4.SA\n",
    "2. ‚úÖ **Engenharia de features**: Cria√ß√£o de 20+ indicadores t√©cnicos\n",
    "3. ‚úÖ **Divis√£o temporal**: Treino/teste respeitando a natureza temporal\n",
    "4. ‚úÖ **M√∫ltiplos modelos**: Compara√ß√£o de diferentes algoritmos\n",
    "5. ‚úÖ **Ensemble learning**: Combina√ß√£o dos melhores modelos\n",
    "6. ‚úÖ **Avalia√ß√£o rigorosa**: M√©tricas completas e visualiza√ß√µes\n",
    "\n",
    "### Pr√≥ximos Passos (se necess√°rio)\n",
    "- **Otimiza√ß√£o de hiperpar√¢metros**: GridSearch mais detalhado\n",
    "- **Mais features**: Dados macroecon√¥micos, sentimento de mercado\n",
    "- **Modelos avan√ßados**: LSTM, Transformer para s√©ries temporais\n",
    "- **Validation**: Walk-forward validation para robustez temporal\n",
    "- **Risk management**: Incorporar stop-loss e take-profit\n",
    "\n",
    "### ‚ö†Ô∏è Avisos Importantes\n",
    "- Este modelo √© para fins educacionais e de pesquisa\n",
    "- Mercados financeiros s√£o imprevis√≠veis e envolvem riscos\n",
    "- Sempre consulte profissionais qualificados antes de investir\n",
    "- Performance passada n√£o garante resultados futuros"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
